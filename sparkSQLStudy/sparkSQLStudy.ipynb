{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @Author: King\\n    @Date: 2019.05.16\\n    @Purpose: Spark入门：DataFrame,SparkSQL\\n    @Introduction:   Spark入门：DataFrame,SparkSQL\\n    @Datasets: \\n    @Link : \\n    @Reference : http://dblab.xmu.edu.cn/blog/1709-2/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.05.16\n",
    "    @Purpose: Spark入门：DataFrame,SparkSQL\n",
    "    @Introduction:   Spark入门：DataFrame,SparkSQL\n",
    "    @Datasets: \n",
    "    @Link : \n",
    "    @Reference : http://dblab.xmu.edu.cn/blog/1709-2/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![作者](../img/bigdata-roadmap.jpg)\n",
    "【版权声明】博客内容由厦门大学数据库实验室拥有版权，未经允许，请勿转载！\n",
    "\n",
    "\n",
    "## 一、 Spark入门：Spark SQL简介(Python版)\n",
    "\n",
    "Spark SQL是Spark生态系统中非常重要的组件，其前身为Shark。Shark是Spark上的数据仓库，最初设计成与Hive兼容，但是该项目于2014年开始停止开发，转向Spark SQL。Spark SQL全面继承了Shark，并进行了优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）、从Shark说起\n",
    "\n",
    "Shark即Hive on Spark，为了实现与Hive兼容，Shark在HiveQL方面重用了Hive中的HiveQL解析、逻辑执行计划翻译、执行计划优化等逻辑，可以近似认为仅将物理执行计划从MapReduce作业替换成了Spark作业，通过Hive的HiveQL解析，把HiveQL翻译成Spark上的RDD操作。（要想了解更多数据仓库Hive的知识，可以参考厦门大学数据库实验室的[Hive授课视频](http://dblab.xmu.edu.cn/post/bigdata-online-course/#lesson8)、[Hive安装指南](http://dblab.xmu.edu.cn/blog/install-hive/)）\n",
    "\n",
    "Shark的设计导致了两个问题：一是执行计划优化完全依赖于Hive，不方便添加新的优化策略；二是因为Spark是线程级并行，而MapReduce是进程级并行，因此，Spark在兼容Hive的实现上存在线程安全问题，导致Shark不得不使用另外一套独立维护的打了补丁的Hive源码分支。\n",
    "\n",
    "Shark的实现继承了大量的Hive代码，因而给优化和维护带来了大量的麻烦，特别是基于MapReduce设计的部分，成为整个项目的瓶颈。因此，在2014年的时候，Shark项目中止，并转向Spark SQL的开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)、Spark SQL设计\n",
    "\n",
    "Spark SQL的架构如图16-12所示，在Shark原有的架构上重写了逻辑执行计划的优化部分，解决了Shark存在的问题。Spark SQL在Hive兼容层面仅依赖HiveQL解析和Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责。\n",
    "\n",
    "![图16-12-Spark-SQL架构](../img/图16-12-Spark-SQL架构.jpg)\n",
    "\n",
    "Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以来自Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据。Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范。从Spark1.2 升级到Spark1.3以后，Spark SQL中的SchemaRDD变为了DataFrame，DataFrame相对于SchemaRDD有了较大改变,同时提供了更多好用且方便的API，如图16-13所示。\n",
    "\n",
    "![图16-13-Spark-SQL支持的数据格式和编程语言](../img/图16-13-Spark-SQL支持的数据格式和编程语言.jpg)\n",
    "\n",
    "Spark SQL可以很好地支持SQL查询，一方面，可以编写Spark应用程序使用SQL语句进行数据查询，另一方面，也可以使用标准的数据库连接器（比如JDBC或ODBC）连接Spark进行SQL查询，这样，一些市场上现有的商业智能工具（比如Tableau）就可以很好地和Spark SQL组合起来使用，从而使得这些外部工具借助于Spark SQL也能获得大规模数据的处理分析能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、 Spark入门：DataFrame与RDD的区别(Python版)\n",
    "\n",
    "DataFrame的推出，让Spark具备了处理大规模结构化数据的能力，不仅比原有的RDD转化方式更加简单易用，而且获得了更高的计算性能。Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询。\n",
    "\n",
    "![DataFrame-RDD.jpg](../img/DataFrame-RDD.jpg)\n",
    "\n",
    "从上面的图中可以看出DataFrame和RDD的区别：\n",
    "\n",
    "（1）RDD是分布式的 Java对象的集合，比如，RDD[Person]是以Person为类型参数，但是，Person类的内部结构对于RDD而言却是不可知的。\n",
    "\n",
    "（2）DataFrame是一种以RDD为基础的分布式数据集，也就是分布式的Row对象的集合（每个Row对象代表一行记录），提供了详细的结构信息，也就是我们经常说的模式（schema），Spark SQL可以清楚地知道该数据集中包含哪些列、每列的名称和类型。\n",
    "\n",
    "<strong>和RDD一样，DataFrame的各种变换操作也采用惰性机制</strong>，只是记录了各种转换的逻辑转换路线图（是一个DAG图），不会发生真正的计算，这个DAG图相当于一个逻辑查询计划，最终，会被翻译成物理查询计划，生成RDD DAG，按照之前介绍的RDD DAG的执行方式去完成最终的计算得到结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、 Spark入门：DataFrame的创建(Python版)\n",
    "\n",
    "从Spark2.0以上版本开始，Spark使用全新的SparkSession接口替代Spark1.6中的SQLContext及HiveContext接口来实现其对数据加载、转换、处理等功能。SparkSession实现了SQLContext及HiveContext所有功能。\n",
    "\n",
    "SparkSession支持从不同的数据源加载数据，并把数据转换成DataFrame，并且支持把DataFrame转换成SQLContext自身中的表，然后使用SQL语句来操作数据。SparkSession亦提供了HiveQL以及其他依赖于Hive的功能的支持。\n",
    "\n",
    "下面我们就介绍如何使用SparkSession来创建DataFrame。\n",
    "\n",
    "首先，请找到样例数据。 Spark已经为我们提供了几个样例数据，就保存在“主目录/resources/”这个目录下，这个目录下有两个样例数据people.json和people.txt。\n",
    "\n",
    "people.json文件的内容如下：\n",
    "\n",
    "```json\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "```\n",
    "people.txt文件的内容如下：\n",
    "\n",
    "```\n",
    "Michael, 29\n",
    "Andy, 30\n",
    "Justin, 19\n",
    "```\n",
    "下面我们就介绍如何从people.json文件中读取数据并生成DataFrame并显示数据（从people.txt文件生成DataFrame需要后面将要介绍的另外一种方式）。\n",
    "\n",
    "#### (1)、 引入 pyspark 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 pyspark 库\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)、SparkSession 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json(\"../resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)、SparkSession 常用操作介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 打印模式信息\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择多列，并对 age+1\n",
    "df.select(df.name,df.age + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 条件过滤,筛选出 age > 20 的数据\n",
    "df.filter(df.age > 20 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组聚合，分组统计\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|sum(age)|\n",
      "+----+--------+\n",
      "|  19|      19|\n",
      "|null|    null|\n",
      "|  30|      30|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组聚合，分组求和\n",
    "df.groupBy(\"age\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  19| Justin|\n",
      "|  30|   Andy|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 排序\n",
    "## 递降\n",
    "df.sort(df.age.desc()).show()\n",
    "## 递增\n",
    "df.sort(df.age.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#多列排序\n",
    "df.sort(df.age.desc(), df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|username| age|\n",
      "+--------+----+\n",
      "| Michael|null|\n",
      "|    Andy|  30|\n",
      "|  Justin|  19|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#对列进行重命名\n",
    "df.select(df.name.alias(\"username\"),df.age).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、 Spark入门：从RDD转换得到DataFrame(Python版)\n",
    "\n",
    "Spark官网提供了两种方法来实现从RDD转换得到DataFrame：\n",
    "\n",
    "第一种方法是，利用反射来推断包含特定类型对象的RDD的schema，适用对已知数据结构的RDD转换；\n",
    "\n",
    "第二种方法是，使用编程接口，构造一个schema并将其应用在已知的RDD上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)、利用反射机制推断RDD模式\n",
    "\n",
    "在利用反射机制推断RDD模式时,我们会用到toDF()方法\n",
    "\n",
    "下面是在pyspark中执行命令以及反馈的信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\progrom\\python\\python\\python3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "d:\\progrom\\python\\python\\python3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['name'] = x[0]\n",
    "    rel['age'] = x[1]\n",
    "    return rel\n",
    "\n",
    "peopleRDD = sc.textFile(\"../resources/people.txt\").map(lambda line : line.split(',')).map(lambda x: Row(**f(x)))\n",
    "\n",
    "peopleDF = peopleRDD.toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Name: 29,Age:Michael', 'Name: 30,Age:Andy', 'Name: 19,Age:Justin']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.createOrReplaceTempView(\"people\")  #必须注册为临时表才能供下面的查询使用\n",
    "\n",
    "personsDF = spark.sql(\"select * from people\")\n",
    "personsDF.show()\n",
    "personsDF.rdd.map(lambda t : \"Name:\"+t[0]+\",\"+\"Age:\"+t[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)、使用编程方式定义RDD模式\n",
    "\n",
    "使用createDataFrame(rdd, schema)编程方式定义RDD模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext( 'local', 'test')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 RDD\n",
    "peopleRDD = sc.textFile(\"../resources/people.txt\")\n",
    "\n",
    "# 定义一个模式字符串\n",
    "schemaString = \"name age\"\n",
    "\n",
    "# 根据模式字符串生成模式\n",
    "fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True), schemaString.split(\" \")))\n",
    "schema = StructType(fields)\n",
    "# 从上面信息可以看出，schema描述了模式信息，模式中包含name和age两个字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowRDD = peopleRDD.map(lambda line : line.split(',')).map(lambda attributes : Row(attributes[0], attributes[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的代码中，peopleRDD.map(lambda line : line.split(‘,’))作用是对people这个RDD中的每一行元素都进行解析。比如，people这个RDD的第一行是：\n",
    "\n",
    "```\n",
    "    Michael, 29\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.createDataFrame(rowRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必须注册为临时表才能供下面查询使用\n",
    "peopleDF.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name: Michael,age: 29', 'name: Andy,age: 30', 'name: Justin,age: 19']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = spark.sql(\"SELECT * FROM people\")\n",
    "results.rdd.map( lambda attributes : \"name: \" + attributes[0]+\",\"+\"age:\"+attributes[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这行内容经过peopleRDD.map(lambda line : line.split(‘,’)).操作后，就得到一个集合{Michael,29}。后面经过map(lambda attributes : Row(attributes[0], attributes[1]))操作时，这时的p就是这个集合{Michael,29}，这时p[0]就是Micheael，p[1]就是29，map(lambda attributes : Row(attributes[0], attributes[1]))就会生成一个Row对象，这个对象里面包含了两个字段的值，这个Row对象就构成了rowRDD中的其中一个元素。因为people有3行文本，所以，最终，rowRDD中会包含3个元素，每个元素都是org.apache.spark.sql.Row类型。实际上，Row对象只是对基本数据类型（比如整型或字符串）的数组的封装，本质就是一个定长的字段数组。\n",
    "peopleDF = spark.createDataFrame(rowRDD, schema)，这条语句就相当于建立了rowRDD数据集和模式之间的对应关系，从而我们就知道对于rowRDD的每行记录，第一个字段的名称是schema中的“name”，第二个字段的名称是schema中的“age”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)、把RDD保存成文件\n",
    "\n",
    "这里介绍如何把RDD保存成文本文件，后面还会介绍其他格式的保存。\n",
    "\n",
    "##### 1) 第一种保存方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.read.format(\"json\").load(\"../resources/people.json\")\n",
    "\n",
    "peopleDF.select(\"name\", \"age\").write.format(\"csv\").save(\"../resources/newpeople.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，这里使用select(“name”, “age”)确定要把哪些列进行保存，然后调用write.format(“csv”).save ()保存成csv文件。在后面小节中，我们还会介绍其他保存方式。\n",
    "\n",
    "另外，write.format()支持输出 json,parquet, jdbc, orc, libsvm, csv, text等格式文件，如果要输出文本文件，可以采用write.format(“text”)，但是，需要注意，只有select()中只存在一个列时，才允许保存成文本文件，如果存在两个列，比如select(“name”, “age”)，就不能保存成文本文件。\n",
    "\n",
    "可以看到 resources 这个目录下面有个newpeople.csv文件夹（注意，不是文件），这个文件夹中包含下面两个文件：\n",
    "\n",
    "```\n",
    "    part-r-00000-33184449-cb15-454c-a30f-9bb43faccac1.csv \n",
    "    _SUCCESS\n",
    "```\n",
    "\n",
    "不用理会_SUCCESS这个文件，只要看一下part-r-00000-33184449-cb15-454c-a30f-9bb43faccac1.csv这个文件，可以用vim编辑器打开这个文件查看它的内容，该文件内容如下：\n",
    "\n",
    "\n",
    "```\n",
    "    Michael,\n",
    "    Andy,30\n",
    "    Justin,19\n",
    "```\n",
    "\n",
    "因为people.json文件中，Michael这个名字不存在对应的age，所以，上面第一行逗号后面没有内容。\n",
    "\n",
    "如果我们要再次把newpeople.csv中的数据加载到RDD中，可以直接使用newpeople.csv目录名称，而不需要使用part-r-00000-33184449-cb15-454c-a30f-9bb43faccac1.csv 文件，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael,\"\"', 'Andy,30', 'Justin,19']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"../resources/newpeople.csv\")\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) 第二种保存方式\n",
    "\n",
    "进入pyspark执行下面命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.read.format(\"json\").load(\"../resources/people.json\")\n",
    "peopleDF.rdd.saveAsTextFile(\"../resources/newpeople.txt\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，我们是把DataFrame转换成RDD，然后调用saveAsTextFile()保存成文本文件。在后面小节中，我们还会介绍其他保存方式。\n",
    "\n",
    "可以看到 resources 这个目录下面有个 newpeople.txt 文件夹（注意，不是文件），这个文件夹中包含下面两个文件：\n",
    "\n",
    "```\n",
    "    part-00000  \n",
    "    _SUCCESS\n",
    "```\n",
    "\n",
    "不用理会_SUCCESS这个文件，只要看一下part-00000这个文件，可以用vim编辑器打开这个文件查看它的内容，该文件内容如下：\n",
    "\n",
    "```\n",
    "    [null,Michael]\n",
    "    [30,Andy]\n",
    "    [19,Justin]\n",
    "```\n",
    "\n",
    "如果我们要再次把newpeople.txt中的数据加载到RDD中，可以直接使用newpeople.txt目录名称，而不需要使用part-00000文件，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Row(age=None, name='Michael')\",\n",
       " \"Row(age=30, name='Andy')\",\n",
       " \"Row(age=19, name='Justin')\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"../resources/newpeople.txt\")\n",
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
