{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @Author: King\\n    @Date: 2019.05.16\\n    @Purpose: Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\\n    @Introduction:   Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\\n    @Datasets: \\n    @Link : \\n    @Reference : Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.05.16\n",
    "    @Purpose: Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\n",
    "    @Introduction:   Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\n",
    "    @Datasets: \n",
    "    @Link : \n",
    "    @Reference : Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![作者](../img/bigdata-roadmap.jpg)\n",
    "【版权声明】博客内容由厦门大学数据库实验室拥有版权，未经允许，请勿转载！\n",
    "\n",
    "\n",
    "## 一、 Spark2.1.0+入门：读写Parquet(DataFrame)(Python版)\n",
    "\n",
    "Spark SQL可以支持Parquet、JSON、Hive等数据源，并且可以通过JDBC连接外部数据源。前面的介绍中，我们已经涉及到了JSON、文本格式的加载，这里不再赘述。这里介绍Parquet，下一节会介绍JDBC数据库连接。\n",
    "\n",
    "Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。Parquet是语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与Parquet配合的组件有：\n",
    "* 查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL\n",
    "* 计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite\n",
    "* 数据模型: Avro, Thrift, Protocol Buffers, POJOs\n",
    "\n",
    "#### (1)、 引入 pyspark 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 pyspark 库\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)、SparkSession 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json(\"../resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)、将DataFrame保存成parquet文件\n",
    "\n",
    "即将 JSON 数据 写入到目录\"../resources/newpeople.parquet\"中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"../resources/newpeople.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面命令执行后，可以看到”../resources”这个目录下多了一个newpeople.parquet，不过，注意，这不是一个文件，而是一个目录（不要被newpeople.parquet中的圆点所迷惑，文件夹名称也可以包含圆点），也就是说，peopleDF.write.parquet(“../resources/newpeople.parquet”)括号里面的参数是文件夹，不是文件名。下面我们可以进入newpeople.parquet目录，会发现下面2个文件：\n",
    "```\n",
    "    part-r-00000-8d3a120f-b3b5-4582-b26b-f3693df80d45.snappy.parquet\n",
    "    _SUCCESS\n",
    "```\n",
    "这2个文件都是刚才保存生成的。现在问题来了，如果我们要再次把这个刚生成的数据又加载到DataFrame中，应该加载哪个文件呢？很简单，只要加载newpeople.parquet目录即可，而不是加载这2个文件，语句如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin')]\n"
     ]
    }
   ],
   "source": [
    "parquetFileDF = spark.read.parquet(\"../resources/newpeople.parquet\")\n",
    "\n",
    "parquetFileDF.createOrReplaceTempView(\"parquetFile\")\n",
    "\n",
    "namesDF = spark.sql(\"SELECT * FROM parquetFile\")\n",
    "\n",
    "namesDF.rdd.foreach(lambda person: print(person.name))\n",
    "\n",
    "print(namesDF.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
