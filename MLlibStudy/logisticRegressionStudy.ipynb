{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @Author: King\\n    @Date: 2019.05.20\\n    @Purpose: Spark2.1.0+入门：6.3 特征抽取、转化和选择\\n    @Introduction:   Spark2.1.0+入门：6.3 特征抽取、转化和选择\\n    @Datasets: \\n    @Link : http://dblab.xmu.edu.cn/blog/1709-2/\\n    @Reference : Spark2.1.0+入门：6.3 特征抽取、转化和选择\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    @Author: King\n",
    "    @Date: 2019.05.20\n",
    "    @Purpose: Spark2.1.0入门：逻辑斯蒂回归分类器(Python版)\n",
    "    @Introduction:  Spark2.1.0入门：逻辑斯蒂回归分类器(Python版)\n",
    "    @Datasets: \n",
    "    @Link : http://dblab.xmu.edu.cn/blog/1773-2/\n",
    "    @Reference : Spark2.1.0入门：逻辑斯蒂回归分类器(Python版)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![作者](../img/bigdata-roadmap.jpg)\n",
    "【版权声明】博客内容由厦门大学数据库实验室拥有版权，未经允许，请勿转载！\n",
    "\n",
    "## 一、逻辑斯蒂回归\n",
    "\n",
    "### 1、方法简介\n",
    "\n",
    "​ 逻辑斯蒂回归（logistic regression）是统计学习中的经典分类方法，属于对数线性模型。logistic回归的因变量可以是二分类的，也可以是多分类的。\n",
    "\n",
    "#### 1)、logistic分布\n",
    "\n",
    "![](../img/logistic1.png)\n",
    "\n",
    "#### 2)、二项logistic回归模型\n",
    "![](../img/logistic2.png)\n",
    "\n",
    "#### 3)、参数估计\n",
    "![](../img/logistic3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、示例代码\n",
    "\n",
    "​ 我们以iris数据集（iris{http://dblab.xmu.edu.cn/blog/wp-content/uploads/2017/03/iris.txt}）\n",
    "为例进行分析。iris以鸢尾花的特征作为数据来源，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性，是在数据挖掘、数据分类中非常常用的测试集、训练集。为了便于理解，我们这里主要用后两个属性（花瓣的长度和宽度）来进行分类。目前 spark.ml 中支持二分类和多分类，我们将分别从“用二项逻辑斯蒂回归来解决二分类问题”、“用多项逻辑斯蒂回归来解决二分类问题”、“用多项逻辑斯蒂回归来解决多分类问题”三个方面进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)、用二项逻辑斯蒂回归解决 二分类 问题\n",
    "\n",
    "首先我们先取其中的后两类数据，用二项逻辑斯蒂回归进行二分类分析。\n",
    "\n",
    "##### step 1、导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 pyspark 库\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row,functions\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer,HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel,BinaryLogisticRegressionSummary, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 2、读取数据，简要分析\n",
    "\n",
    "​ 我们定制一个函数，来返回一个指定的数据，然后读取文本文件，第一个map把每行的数据用“,”隔开，比如在我们的数据集中，每行被分成了5部分，前4部分是鸢尾花的4个特征，最后一部分是鸢尾花的分类；我们这里把特征存储在Vector中，创建一个Iris模式的RDD，然后转化成dataframe；最后调用show()方法来查看一下部分数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|         features|      label|\n",
      "+-----------------+-----------+\n",
      "|[5.1,3.5,1.4,0.2]|Iris-setosa|\n",
      "|[4.9,3.0,1.4,0.2]|Iris-setosa|\n",
      "|[4.7,3.2,1.3,0.2]|Iris-setosa|\n",
      "|[4.6,3.1,1.5,0.2]|Iris-setosa|\n",
      "|[5.0,3.6,1.4,0.2]|Iris-setosa|\n",
      "|[5.4,3.9,1.7,0.4]|Iris-setosa|\n",
      "|[4.6,3.4,1.4,0.3]|Iris-setosa|\n",
      "|[5.0,3.4,1.5,0.2]|Iris-setosa|\n",
      "|[4.4,2.9,1.4,0.2]|Iris-setosa|\n",
      "|[4.9,3.1,1.5,0.1]|Iris-setosa|\n",
      "|[5.4,3.7,1.5,0.2]|Iris-setosa|\n",
      "|[4.8,3.4,1.6,0.2]|Iris-setosa|\n",
      "|[4.8,3.0,1.4,0.1]|Iris-setosa|\n",
      "|[4.3,3.0,1.1,0.1]|Iris-setosa|\n",
      "|[5.8,4.0,1.2,0.2]|Iris-setosa|\n",
      "|[5.7,4.4,1.5,0.4]|Iris-setosa|\n",
      "|[5.4,3.9,1.3,0.4]|Iris-setosa|\n",
      "|[5.1,3.5,1.4,0.3]|Iris-setosa|\n",
      "|[5.7,3.8,1.7,0.3]|Iris-setosa|\n",
      "|[5.1,3.8,1.5,0.3]|Iris-setosa|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    " \n",
    "data = spark.sparkContext.textFile(\"../resources/iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p))).toDF()\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​ 因为我们现在处理的是2分类问题，所以我们不需要全部的3类数据，我们要从中选出两类的数据。这里首先把刚刚得到的数据注册成一个表iris，注册成这个表之后，我们就可以通过sql语句进行数据查询，比如我们这里选出了所有不属于“Iris-setosa”类别的数据；选出我们需要的数据后，我们可以把结果打印出来看一下，这时就已经没有“Iris-setosa”类别的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-versicolor:[7.0,3.2,4.7,1.4]\n",
      "Iris-versicolor:[6.4,3.2,4.5,1.5]\n",
      "Iris-versicolor:[6.9,3.1,4.9,1.5]\n",
      "Iris-versicolor:[5.5,2.3,4.0,1.3]\n",
      "Iris-versicolor:[6.5,2.8,4.6,1.5]\n",
      "Iris-versicolor:[5.7,2.8,4.5,1.3]\n",
      "Iris-versicolor:[6.3,3.3,4.7,1.6]\n",
      "Iris-versicolor:[4.9,2.4,3.3,1.0]\n",
      "Iris-versicolor:[6.6,2.9,4.6,1.3]\n",
      "Iris-versicolor:[5.2,2.7,3.9,1.4]\n",
      "Iris-versicolor:[5.0,2.0,3.5,1.0]\n",
      "Iris-versicolor:[5.9,3.0,4.2,1.5]\n",
      "Iris-versicolor:[6.0,2.2,4.0,1.0]\n",
      "Iris-versicolor:[6.1,2.9,4.7,1.4]\n",
      "Iris-versicolor:[5.6,2.9,3.6,1.3]\n",
      "Iris-versicolor:[6.7,3.1,4.4,1.4]\n",
      "Iris-versicolor:[5.6,3.0,4.5,1.5]\n",
      "Iris-versicolor:[5.8,2.7,4.1,1.0]\n",
      "Iris-versicolor:[6.2,2.2,4.5,1.5]\n",
      "Iris-versicolor:[5.6,2.5,3.9,1.1]\n",
      "Iris-versicolor:[5.9,3.2,4.8,1.8]\n",
      "Iris-versicolor:[6.1,2.8,4.0,1.3]\n",
      "Iris-versicolor:[6.3,2.5,4.9,1.5]\n",
      "Iris-versicolor:[6.1,2.8,4.7,1.2]\n",
      "Iris-versicolor:[6.4,2.9,4.3,1.3]\n",
      "Iris-versicolor:[6.6,3.0,4.4,1.4]\n",
      "Iris-versicolor:[6.8,2.8,4.8,1.4]\n",
      "Iris-versicolor:[6.7,3.0,5.0,1.7]\n",
      "Iris-versicolor:[6.0,2.9,4.5,1.5]\n",
      "Iris-versicolor:[5.7,2.6,3.5,1.0]\n",
      "Iris-versicolor:[5.5,2.4,3.8,1.1]\n",
      "Iris-versicolor:[5.5,2.4,3.7,1.0]\n",
      "Iris-versicolor:[5.8,2.7,3.9,1.2]\n",
      "Iris-versicolor:[6.0,2.7,5.1,1.6]\n",
      "Iris-versicolor:[5.4,3.0,4.5,1.5]\n",
      "Iris-versicolor:[6.0,3.4,4.5,1.6]\n",
      "Iris-versicolor:[6.7,3.1,4.7,1.5]\n",
      "Iris-versicolor:[6.3,2.3,4.4,1.3]\n",
      "Iris-versicolor:[5.6,3.0,4.1,1.3]\n",
      "Iris-versicolor:[5.5,2.5,4.0,1.3]\n",
      "Iris-versicolor:[5.5,2.6,4.4,1.2]\n",
      "Iris-versicolor:[6.1,3.0,4.6,1.4]\n",
      "Iris-versicolor:[5.8,2.6,4.0,1.2]\n",
      "Iris-versicolor:[5.0,2.3,3.3,1.0]\n",
      "Iris-versicolor:[5.6,2.7,4.2,1.3]\n",
      "Iris-versicolor:[5.7,3.0,4.2,1.2]\n",
      "Iris-versicolor:[5.7,2.9,4.2,1.3]\n",
      "Iris-versicolor:[6.2,2.9,4.3,1.3]\n",
      "Iris-versicolor:[5.1,2.5,3.0,1.1]\n",
      "Iris-versicolor:[5.7,2.8,4.1,1.3]\n",
      "Iris-virginica:[6.3,3.3,6.0,2.5]\n",
      "Iris-virginica:[5.8,2.7,5.1,1.9]\n",
      "Iris-virginica:[7.1,3.0,5.9,2.1]\n",
      "Iris-virginica:[6.3,2.9,5.6,1.8]\n",
      "Iris-virginica:[6.5,3.0,5.8,2.2]\n",
      "Iris-virginica:[7.6,3.0,6.6,2.1]\n",
      "Iris-virginica:[4.9,2.5,4.5,1.7]\n",
      "Iris-virginica:[7.3,2.9,6.3,1.8]\n",
      "Iris-virginica:[6.7,2.5,5.8,1.8]\n",
      "Iris-virginica:[7.2,3.6,6.1,2.5]\n",
      "Iris-virginica:[6.5,3.2,5.1,2.0]\n",
      "Iris-virginica:[6.4,2.7,5.3,1.9]\n",
      "Iris-virginica:[6.8,3.0,5.5,2.1]\n",
      "Iris-virginica:[5.7,2.5,5.0,2.0]\n",
      "Iris-virginica:[5.8,2.8,5.1,2.4]\n",
      "Iris-virginica:[6.4,3.2,5.3,2.3]\n",
      "Iris-virginica:[6.5,3.0,5.5,1.8]\n",
      "Iris-virginica:[7.7,3.8,6.7,2.2]\n",
      "Iris-virginica:[7.7,2.6,6.9,2.3]\n",
      "Iris-virginica:[6.0,2.2,5.0,1.5]\n",
      "Iris-virginica:[6.9,3.2,5.7,2.3]\n",
      "Iris-virginica:[5.6,2.8,4.9,2.0]\n",
      "Iris-virginica:[7.7,2.8,6.7,2.0]\n",
      "Iris-virginica:[6.3,2.7,4.9,1.8]\n",
      "Iris-virginica:[6.7,3.3,5.7,2.1]\n",
      "Iris-virginica:[7.2,3.2,6.0,1.8]\n",
      "Iris-virginica:[6.2,2.8,4.8,1.8]\n",
      "Iris-virginica:[6.1,3.0,4.9,1.8]\n",
      "Iris-virginica:[6.4,2.8,5.6,2.1]\n",
      "Iris-virginica:[7.2,3.0,5.8,1.6]\n",
      "Iris-virginica:[7.4,2.8,6.1,1.9]\n",
      "Iris-virginica:[7.9,3.8,6.4,2.0]\n",
      "Iris-virginica:[6.4,2.8,5.6,2.2]\n",
      "Iris-virginica:[6.3,2.8,5.1,1.5]\n",
      "Iris-virginica:[6.1,2.6,5.6,1.4]\n",
      "Iris-virginica:[7.7,3.0,6.1,2.3]\n",
      "Iris-virginica:[6.3,3.4,5.6,2.4]\n",
      "Iris-virginica:[6.4,3.1,5.5,1.8]\n",
      "Iris-virginica:[6.0,3.0,4.8,1.8]\n",
      "Iris-virginica:[6.9,3.1,5.4,2.1]\n",
      "Iris-virginica:[6.7,3.1,5.6,2.4]\n",
      "Iris-virginica:[6.9,3.1,5.1,2.3]\n",
      "Iris-virginica:[5.8,2.7,5.1,1.9]\n",
      "Iris-virginica:[6.8,3.2,5.9,2.3]\n",
      "Iris-virginica:[6.7,3.3,5.7,2.5]\n",
      "Iris-virginica:[6.7,3.0,5.2,2.3]\n",
      "Iris-virginica:[6.3,2.5,5.0,1.9]\n",
      "Iris-virginica:[6.5,3.0,5.2,2.0]\n",
      "Iris-virginica:[6.2,3.4,5.4,2.3]\n",
      "Iris-virginica:[5.9,3.0,5.1,1.8]\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"iris\")\n",
    "df = spark.sql(\"select * from iris where label != 'Iris-setosa'\")\n",
    "rel = df.rdd.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 3、构建ML的pipeline\n",
    "\n",
    "分别获取标签列和特征列，进行索引，并进行了重命名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们把数据集随机分成训练集和测试集，其中训练集占70%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 然后，我们设置logistic的参数，这里我们统一用setter的方法来设置，也可以用ParamMap来设置（具体的可以查看spark mllib的官网）。这里我们设置了循环次数为10次，正则化项为0.3等，具体的可以设置的参数可以通过explainParams()来获取，还能看到我们已经设置的参数的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: indexedFeatures)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: indexedLabel)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​ 这里我们设置一个labelConverter，目的是把预测的类别重新转化成字符型的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-versicolor', 'Iris-virginica']\n",
      "prediction\n",
      "predictedLabel\n"
     ]
    }
   ],
   "source": [
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "\n",
    "print(labelConverter.getLabels())\n",
    "print(labelConverter.getInputCol())\n",
    "print(labelConverter.getOutputCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​ 构建pipeline，设置stage，然后调用fit()来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPipeline =  Pipeline().setStages([labelIndexer, featureIndexer, lr, labelConverter])\n",
    "lrPipelineModel = lrPipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​ pipeline本质上是一个Estimator，当pipeline调用fit()的时候就产生了一个PipelineModel，本质上是一个Transformer。然后这个PipelineModel就可以调用transform()来进行预测，生成一个新的DataFrame，即利用训练得到的模型对测试集进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictions = lrPipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​ 最后我们可以输出预测的结果，其中select选择要输出的列，collect获取所有行的数据，用foreach把每行打印出来。其中打印出来的值依次分别代表该行数据的真实分类和特征值、预测属于不同分类的概率、预测的分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-virginica,[4.9,2.5,4.5,1.7]-->prob=[0.5274710033427651,0.47252899665723486],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[5.0,2.0,3.5,1.0]-->prob=[0.6494390687143794,0.3505609312856206],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[5.2,2.7,3.9,1.4]-->prob=[0.5819241306479392,0.4180758693520607],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[5.7,2.8,4.5,1.3]-->prob=[0.6010403639407159,0.39895963605928414],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[5.7,2.9,4.2,1.3]-->prob=[0.6010403639407159,0.39895963605928414],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[5.8,2.7,3.9,1.2]-->prob=[0.6185415975365418,0.38145840246345813],predictedLabelIris-versicolor\n",
      "Iris-virginica,[5.8,2.7,5.1,1.9]-->prob=[0.4945436926501766,0.5054563073498235],predictedLabelIris-virginica\n",
      "Iris-virginica,[5.8,2.7,5.1,1.9]-->prob=[0.4945436926501766,0.5054563073498235],predictedLabelIris-virginica\n",
      "Iris-versicolor,[5.9,3.0,4.2,1.5]-->prob=[0.5666617965788115,0.4333382034211884],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.0,2.7,5.1,1.6]-->prob=[0.5492065401754112,0.4507934598245889],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.0,2.9,4.5,1.5]-->prob=[0.567003327040756,0.432996672959244],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.1,2.8,4.7,1.2]-->prob=[0.6195257011263475,0.38047429887365236],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.1,2.9,4.7,1.4]-->prob=[0.5849666405689994,0.41503335943100067],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.3,2.5,5.0,1.9]-->prob=[0.49628226149601407,0.5037177385039859],predictedLabelIris-virginica\n",
      "Iris-virginica,[6.3,2.8,5.1,1.5]-->prob=[0.5680275352456152,0.4319724647543848],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.4,2.7,5.3,1.9]-->prob=[0.49662998739950265,0.5033700126004974],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.4,3.2,4.5,1.5]-->prob=[0.568368809212884,0.431631190787116],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.5,3.0,5.2,2.0]-->prob=[0.47894767878134703,0.521052321218653],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.6,3.0,4.4,1.4]-->prob=[0.5866541440163051,0.4133458559836948],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.7,2.5,5.8,1.8]-->prob=[0.5157104480443038,0.4842895519556962],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.8,2.8,4.8,1.4]-->prob=[0.5873285782952685,0.4126714217047316],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.9,3.1,4.9,1.5]-->prob=[0.5700741992695805,0.4299258007304196],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[7.0,3.2,4.7,1.4]-->prob=[0.5880026849552081,0.41199731504479187],predictedLabelIris-versicolor\n",
      "Iris-virginica,[7.1,3.0,5.9,2.1]-->prob=[0.46304673369053057,0.5369532663094695],predictedLabelIris-virginica\n",
      "Iris-virginica,[7.2,3.2,6.0,1.8]-->prob=[0.5174472516195706,0.48255274838042944],predictedLabelIris-versicolor\n",
      "Iris-virginica,[7.7,3.8,6.7,2.2]-->prob=[0.44722059168485145,0.5527794083151485],predictedLabelIris-virginica\n"
     ]
    }
   ],
   "source": [
    "preRel = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in preRel:\n",
    "    print(str(item['label'])+','+str(item['features'])+'-->prob='+str(item['probability'])+',predictedLabel'+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 4、模型评估\n",
    "\n",
    "​ 创建一个MulticlassClassificationEvaluator实例，用setter方法把预测分类的列名和真实分类的列名进行设置；然后计算预测准确率和错误率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.16189039718451492\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\")\n",
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "print(\"Test Error = \" + str(1.0 - lrAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​ 从上面可以看到预测的准确性达到65%，接下来我们可以通过model来获取我们训练得到的逻辑斯蒂模型。前面已经说过model是一个PipelineModel，因此我们可以通过调用它的stages来获取模型，具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.01390973547275124,0.0,0.0,0.07216981898787578]Intercept: -0.5470258093340817numClasses: 2numFeatures: 4\n"
     ]
    }
   ],
   "source": [
    "lrModel = lrPipelineModel.stages[2]\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients)+\"Intercept: \"+str(lrModel.intercept)+\"numClasses: \"+str(lrModel.numClasses)+\"numFeatures: \"+str(lrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 5、模型评估\n",
    "\n",
    "​ spark的ml库还提供了一个对模型的摘要总结（summary），不过目前只支持二项逻辑斯蒂回归，而且要显示转化成BinaryLogisticRegressionSummary 。在下面的代码中，首先获得二项逻辑斯模型的摘要；然后获得10次循环中损失函数的变化，并将结果打印出来，可以看到损失函数随着循环是逐渐变小的，损失函数越小，模型就越好；接下来，我们把摘要强制转化为BinaryLogisticRegressionSummary ，来获取用来评估模型性能的矩阵；通过获取ROC，我们可以判断模型的好坏，areaUnderROC达到了 0.969551282051282，说明我们的分类器还是不错的；最后，我们通过最大化fMeasure来选取最合适的阈值，其中fMeasure是一个综合了召回率和准确率的指标，通过最大化fMeasure，我们可以选取到用来分类的最合适的阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6916855479178757\n",
      "0.6892045259636208\n",
      "0.686634185737353\n",
      "0.67860800539271\n",
      "0.673735712819139\n",
      "0.6693014645935897\n",
      "0.6691009840364661\n",
      "0.6689601483226956\n",
      "0.6675239026310226\n",
      "0.6678712776382452\n",
      "0.6616961853860587\n",
      "0.9787545787545787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_3dfca8e44cbd"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "for item in objectiveHistory:\n",
    "    print(item)\n",
    "    \n",
    "print(trainingSummary.areaUnderROC)\n",
    "\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "\n",
    "maxFMeasure = fMeasure.select(functions.max(\"F-Measure\")).head()[0]\n",
    "\n",
    "bestThreshold = fMeasure.where(fMeasure[\"F-Measure\"]== maxFMeasure).select(\"threshold\").head()[0]\n",
    "\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 6、用多项逻辑斯蒂回归解决 二分类 问题\n",
    "\n",
    "​ 对于二分类问题，我们还可以用多项逻辑斯蒂回归进行多分类分析。多项逻辑斯蒂回归与二项逻辑斯蒂回归类似，只是在模型设置上把family参数设置成multinomial，这里我们仅列出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iris-virginica,[4.9,2.5,4.5,1.7])-->prob=[0.5274710033427651,0.47252899665723486],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.0,2.0,3.5,1.0])-->prob=[0.6494390687143794,0.3505609312856206],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.2,2.7,3.9,1.4])-->prob=[0.5819241306479392,0.4180758693520607],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.7,2.8,4.5,1.3])-->prob=[0.6010403639407159,0.39895963605928414],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.7,2.9,4.2,1.3])-->prob=[0.6010403639407159,0.39895963605928414],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.8,2.7,3.9,1.2])-->prob=[0.6185415975365418,0.38145840246345813],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[5.8,2.7,5.1,1.9])-->prob=[0.4945436926501766,0.5054563073498235],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[5.8,2.7,5.1,1.9])-->prob=[0.4945436926501766,0.5054563073498235],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[5.9,3.0,4.2,1.5])-->prob=[0.5666617965788115,0.4333382034211884],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.0,2.7,5.1,1.6])-->prob=[0.5492065401754112,0.4507934598245889],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.0,2.9,4.5,1.5])-->prob=[0.567003327040756,0.432996672959244],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.1,2.8,4.7,1.2])-->prob=[0.6195257011263475,0.38047429887365236],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.1,2.9,4.7,1.4])-->prob=[0.5849666405689994,0.41503335943100067],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.3,2.5,5.0,1.9])-->prob=[0.49628226149601407,0.5037177385039859],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[6.3,2.8,5.1,1.5])-->prob=[0.5680275352456152,0.4319724647543848],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.4,2.7,5.3,1.9])-->prob=[0.49662998739950265,0.5033700126004974],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.4,3.2,4.5,1.5])-->prob=[0.568368809212884,0.431631190787116],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.5,3.0,5.2,2.0])-->prob=[0.47894767878134703,0.521052321218653],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.6,3.0,4.4,1.4])-->prob=[0.5866541440163051,0.4133458559836948],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.7,2.5,5.8,1.8])-->prob=[0.5157104480443038,0.4842895519556962],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.8,2.8,4.8,1.4])-->prob=[0.5873285782952685,0.4126714217047316],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.9,3.1,4.9,1.5])-->prob=[0.5700741992695805,0.4299258007304196],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[7.0,3.2,4.7,1.4])-->prob=[0.5880026849552081,0.41199731504479187],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[7.1,3.0,5.9,2.1])-->prob=[0.46304673369053057,0.5369532663094695],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[7.2,3.2,6.0,1.8])-->prob=[0.5174472516195706,0.48255274838042944],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[7.7,3.8,6.7,2.2])-->prob=[0.44722059168485145,0.5527794083151485],predictLabel=Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "mlr =  LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8).setFamily(\"multinomial\")\n",
    " \n",
    "mlrPipeline = Pipeline().setStages([labelIndexer, featureIndexer, mlr, labelConverter])\n",
    " \n",
    "mlrPipelineModel = mlrPipeline.fit(trainingData)\n",
    " \n",
    "mlrPreRel = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in mlrPreRel:\n",
    "    print('('+str(item['label'])+','+str(item['features'])+')-->prob='+str(item['probability'])+',predictLabel='+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.16189039718451492\n",
      "Multinomial coefficients: DenseMatrix([[ 0.0296415 ,  0.        ,  0.        , -0.04763024],\n",
      "             [-0.0296415 ,  0.        ,  0.        ,  0.04763024]])Multinomial intercepts: [-0.008250471976582976,0.008250471976582976]numClasses: 2numFeatures: 4\n"
     ]
    }
   ],
   "source": [
    "mlrAccuracy = evaluator.evaluate(lrPredictions)\n",
    " \n",
    "print(\"Test Error = \" + str(1.0 - mlrAccuracy))\n",
    "\n",
    "mlrModel = mlrPipelineModel.stages[2]\n",
    " \n",
    "print(\"Multinomial coefficients: \" +str(mlrModel.coefficientMatrix)+\"Multinomial intercepts: \"+str(mlrModel.interceptVector)+\"numClasses: \"+str(mlrModel.numClasses)+\"numFeatures: \"+str(mlrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 7、用多项逻辑斯蒂回归解决 多分类 问题\n",
    "\n",
    "​ 对于多分类问题，我们需要用多项逻辑斯蒂回归进行多分类分析。这里我们用全部的iris数据集，即有三个类别，过程与上述基本一致，这里我们同样仅列出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iris-virginica,[4.9,2.5,4.5,1.7])-->prob=[0.5274710033427651,0.47252899665723486],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.0,2.0,3.5,1.0])-->prob=[0.6494390687143794,0.3505609312856206],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.2,2.7,3.9,1.4])-->prob=[0.5819241306479392,0.4180758693520607],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.7,2.8,4.5,1.3])-->prob=[0.6010403639407159,0.39895963605928414],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.7,2.9,4.2,1.3])-->prob=[0.6010403639407159,0.39895963605928414],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.8,2.7,3.9,1.2])-->prob=[0.6185415975365418,0.38145840246345813],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[5.8,2.7,5.1,1.9])-->prob=[0.4945436926501766,0.5054563073498235],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[5.8,2.7,5.1,1.9])-->prob=[0.4945436926501766,0.5054563073498235],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[5.9,3.0,4.2,1.5])-->prob=[0.5666617965788115,0.4333382034211884],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.0,2.7,5.1,1.6])-->prob=[0.5492065401754112,0.4507934598245889],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.0,2.9,4.5,1.5])-->prob=[0.567003327040756,0.432996672959244],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.1,2.8,4.7,1.2])-->prob=[0.6195257011263475,0.38047429887365236],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.1,2.9,4.7,1.4])-->prob=[0.5849666405689994,0.41503335943100067],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.3,2.5,5.0,1.9])-->prob=[0.49628226149601407,0.5037177385039859],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[6.3,2.8,5.1,1.5])-->prob=[0.5680275352456152,0.4319724647543848],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.4,2.7,5.3,1.9])-->prob=[0.49662998739950265,0.5033700126004974],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.4,3.2,4.5,1.5])-->prob=[0.568368809212884,0.431631190787116],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.5,3.0,5.2,2.0])-->prob=[0.47894767878134703,0.521052321218653],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.6,3.0,4.4,1.4])-->prob=[0.5866541440163051,0.4133458559836948],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.7,2.5,5.8,1.8])-->prob=[0.5157104480443038,0.4842895519556962],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.8,2.8,4.8,1.4])-->prob=[0.5873285782952685,0.4126714217047316],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.9,3.1,4.9,1.5])-->prob=[0.5700741992695805,0.4299258007304196],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[7.0,3.2,4.7,1.4])-->prob=[0.5880026849552081,0.41199731504479187],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[7.1,3.0,5.9,2.1])-->prob=[0.46304673369053057,0.5369532663094695],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[7.2,3.2,6.0,1.8])-->prob=[0.5174472516195706,0.48255274838042944],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[7.7,3.8,6.7,2.2])-->prob=[0.44722059168485145,0.5527794083151485],predictLabel=Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "mlrPreRel = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in mlrPreRel:\n",
    "    print('('+str(item['label'])+','+str(item['features'])+')-->prob='+str(item['probability'])+',predictLabel='+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.16189039718451492\n",
      "Multinomial coefficients: DenseMatrix([[-0.01390974,  0.        ,  0.        ,  0.07216982]])\n",
      "Multinomial intercepts: [-0.5470258093340817]numClasses: 2numFeatures: 4\n"
     ]
    }
   ],
   "source": [
    "mlrAccuracy = evaluator.evaluate(lrPredictions)\n",
    " \n",
    "print(\"Test Error = \" + str(1.0 - mlrAccuracy))\n",
    "\n",
    " \n",
    "mlrModel = lrPipelineModel.stages[2]\n",
    " \n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix)+\"Multinomial intercepts: \"+str(mlrModel.interceptVector)+\"numClasses: \"+str(mlrModel.numClasses)+\"numFeatures: \"+str(mlrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
